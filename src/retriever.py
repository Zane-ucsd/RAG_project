import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
VECTOR_INDEX_PATH = os.path.join(DATA_DIR, "vector.index")
DOCS_PATH = os.path.join(DATA_DIR, "docs.npy")

# åŠ è½½åµŒå…¥æ¨¡å‹
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

def retrieve_top_k(query, k=2):
    """ æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ K ä¸ªæ–‡æ¡£ """
    # åŠ è½½ç´¢å¼•å’Œæ–‡æ¡£
    index = faiss.read_index(VECTOR_INDEX_PATH)
    documents = np.load(DOCS_PATH, allow_pickle=True)

    # è®¡ç®—æŸ¥è¯¢çš„å‘é‡
    query_embedding = embed_model.encode([query], convert_to_numpy=True)

    # åœ¨ FAISS ä¸­æœç´¢æœ€ç›¸å…³çš„ K ä¸ªæ–‡æ¡£
    _, retrieved_indices = index.search(query_embedding, k)
    retrieved_docs = [documents[idx] for idx in retrieved_indices[0]]

    return retrieved_docs

if __name__ == "__main__":
    query = "å°é’é©¬æ˜¯äººè¿˜æ˜¯ä¸œè¥¿ï¼Ÿ"
    retrieved_docs = retrieve_top_k(query)
    print(f"ğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£: {retrieved_docs}")
