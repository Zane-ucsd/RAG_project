from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# âœ… é€‰æ‹© DeepSeek-V2.5 ä½œä¸º LLM
model_name = "deepseek-ai/Janus-Pro-1B"

# âœ… 4-bit é‡åŒ–é…ç½®ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
quant_config = BitsAndBytesConfig(
    
    load_in_4bit=True,  # å¯ç”¨ 4-bit é‡åŒ–
    bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä»ä½¿ç”¨ FP16
    bnb_4bit_use_double_quant=True,  # è¿›ä¸€æ­¥å‹ç¼©
)

# âœ… åŠ è½½ DeepSeek-V2.5 Tokenizerï¼ˆæ·»åŠ  trust_remote_code=Trueï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# âœ… åŠ è½½ DeepSeek-V2.5 æ¨¡å‹ï¼ˆ4-bit é‡åŒ– & trust_remote_code=Trueï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    device_map={"": "cpu"},  # å¼ºåˆ¶åœ¨ CPU è¿è¡Œ
    trust_remote_code=True
)

def generate_answer(query, context):
    """ä½¿ç”¨ DeepSeek-V2.5 ç”Ÿæˆç­”æ¡ˆ"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
    {context}

    é—®é¢˜ï¼š{query}
    ç­”æ¡ˆï¼š
    """

    # âœ… Tokenization
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(model.device)

    # âœ… æ¨¡å‹æ¨ç†
    output = model.generate(**inputs, max_new_tokens=200)

    # âœ… è§£ç è¾“å‡º
    answer = tokenizer.decode(output[0], skip_special_tokens=True)

    return answer



if __name__ == "__main__":
    test_query = "å°é’é©¬æ˜¯äººè¿˜æ˜¯ä¸œè¥¿ï¼Ÿ"
    test_context = "æ¨æ”¿ç†¹ï¼Œå‘¨å­æ¶µï¼Œå°é’é©¬éƒ½æ˜¯ucsd è®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿã€‚"
    print(f"ğŸ¤– ç”Ÿæˆçš„ç­”æ¡ˆ: {generate_answer(test_query, test_context)}")
